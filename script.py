import requests
import logging
import subprocess
import re
import time
import urllib3
import os
from concurrent.futures import ThreadPoolExecutor
from typing import Dict, List, Tuple

# ç¦ç”¨SSLè­¦å‘Š
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# é…ç½®æ—¥å¿—è®°å½•
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def check_ffmpeg():
    """æ£€æŸ¥ffmpegæ˜¯å¦å¯ç”¨"""
    try:
        subprocess.run(['ffmpeg', '-version'], stdout=subprocess.PIPE, stderr=subprocess.PIPE)
        return True
    except FileNotFoundError:
        logging.error("æœªæ‰¾åˆ°ffmpegã€‚è¯·ç¡®ä¿ffmpegå·²æ­£ç¡®å®‰è£…å¹¶æ·»åŠ åˆ°ç³»ç»ŸPATHä¸­ã€‚")
        return False

def check_url_validity(url):
    try:
        response = requests.head(url, timeout=10, verify=False)
        response.raise_for_status()
        return True
    except requests.exceptions.RequestException as e:
        logging.error(f"URL {url} is invalid or unreachable: {e}")
        return False

def fetch_content(url):
    try:
        logging.info(f"Fetching content from {url}")
        response = requests.get(url, timeout=10, verify=False)
        response.raise_for_status()
        return response.content.decode('utf-8-sig')
    except requests.exceptions.RequestException as e:
        logging.error(f"Failed to fetch content from {url}: {e}")
        return None

def filter_content(content):
    if content is None:
        return []
    keywords = ["ãŠ™VIPæµ‹è¯•", "å…³æ³¨å…¬ä¼—å·", "å¤©å¾®ç§‘æŠ€", "è·å–æµ‹è¯•å¯†ç ", "æ›´æ–°æ—¶é—´", "â™¥èšç©ç›’å­", "ğŸŒ¹é˜²å¤±è”","ğŸ“¡  æ›´æ–°æ—¥æœŸ","ğŸ‘‰",]
    return [line for line in content.splitlines() if not any(keyword in line for keyword in keywords)]

def check_stream_quality(url) -> Tuple[bool, float]:
    """æ£€æŸ¥æµåª’ä½“è´¨é‡ï¼Œè¿”å›(æ˜¯å¦å¯ç”¨, è´¨é‡åˆ†æ•°)"""
    if not check_ffmpeg():
        raise RuntimeError("ffmpegå·¥å…·æœªå®‰è£…ï¼Œæ— æ³•è¿›è¡Œæµç•…åº¦æ£€æµ‹")

    try:
        # ä½¿ç”¨ffmpegæ£€æŸ¥æµåª’ä½“è´¨é‡
        # -v quiet: å‡å°‘è¾“å‡º
        # -stats: æ˜¾ç¤ºè¿›åº¦ç»Ÿè®¡
        # -i: è¾“å…¥æ–‡ä»¶
        # -t 10: åªè¯»å–10ç§’
        # -filter:v fps=fps=1: è®¾ç½®å¸§ç‡è¿‡æ»¤å™¨
        command = [
            'ffmpeg',
            '-v', 'quiet',
            '-stats',
            '-i', url,
            '-t', '10',
            '-filter:v', 'fps=fps=1',
            '-f', 'null',
            '-'
        ]

        start_time = time.time()
        process = subprocess.Popen(
            command,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            universal_newlines=True
        )
        
        stdout, stderr = process.communicate(timeout=20)
        duration = time.time() - start_time
        
        if process.returncode == 0:
            # æå–è§†é¢‘ä¿¡æ¯
            bitrate_match = re.search(r'bitrate[=:]\s*(\d+)\s*kb/s', stderr)
            fps_match = re.search(r'(\d+(?:\.\d+)?)\s*fps', stderr)
            resolution_match = re.search(r'(\d+x\d+)', stderr)
            
            # è®¡ç®—è´¨é‡åˆ†æ•° (åŸºäºå¤šä¸ªæŒ‡æ ‡)
            quality_score = 0.0
            
            # æ¯”ç‰¹ç‡å¾—åˆ† (æœ€é«˜40åˆ†)
            if bitrate_match:
                bitrate = float(bitrate_match.group(1))
                quality_score += min(bitrate / 100, 40)  # 4000kbps å¯å¾—åˆ°æœ€é«˜åˆ†
            
            # å¸§ç‡å¾—åˆ† (æœ€é«˜30åˆ†)
            if fps_match:
                fps = float(fps_match.group(1))
                quality_score += min(fps, 30)  # 30fps å¯å¾—åˆ°æœ€é«˜åˆ†
            
            # åˆ†è¾¨ç‡å¾—åˆ† (æœ€é«˜20åˆ†)
            if resolution_match:
                width, height = map(int, resolution_match.group(1).split('x'))
                resolution_score = (width * height) / (1920 * 1080) * 20
                quality_score += min(resolution_score, 20)
            
            # å“åº”æ—¶é—´å¾—åˆ† (æœ€é«˜10åˆ†)
            quality_score += max(0, 10 - duration)  # å“åº”è¶Šå¿«åˆ†æ•°è¶Šé«˜
            
            logging.info(f"Stream quality check passed for {url} with score {quality_score}")
            return True, quality_score
        else:
            logging.error(f"Stream {url} is not playable: {stderr}")
            return False, 0.0
            
    except subprocess.TimeoutExpired:
        logging.error(f"Stream {url} check timed out")
        return False, 0.0
    except Exception as e:
        logging.error(f"Error checking stream {url}: {e}")
        return False, 0.0

def extract_channel_name(line: str) -> str:
    # å°è¯•ä»è¡Œä¸­æå–é¢‘é“åç§°
    if ',' in line:
        return line.split(',')[1].strip()
    elif '#' in line:
        return line.split('#')[1].strip()
    return "Unknown Channel"

def fetch_and_filter(urls):
    stream_data: Dict[str, Tuple[str, float]] = {}  # {é¢‘é“å: (å®Œæ•´è¡Œ, è´¨é‡åˆ†æ•°)}
    
    with ThreadPoolExecutor() as executor:
        # é¦–å…ˆæ£€æŸ¥URLçš„æœ‰æ•ˆæ€§
        valid_urls = [url for url in urls if check_url_validity(url)]
        # ç„¶åè·å–å†…å®¹
        results = list(executor.map(fetch_content, valid_urls))
    
    filtered_lines = []
    for content in results:
        filtered_lines.extend(filter_content(content))
    
    # æ£€æŸ¥æ¯ä¸ªç›´æ’­æºçš„å¯ç”¨æ€§å’Œè´¨é‡
    with ThreadPoolExecutor(max_workers=5) as executor:
        for line in filtered_lines:
            if line.startswith('http'):
                url = line.split()[0]  # æå–URLéƒ¨åˆ†
                channel_name = extract_channel_name(line)
                is_valid, quality_score = check_stream_quality(url)
                
                if is_valid:
                    if channel_name in stream_data:
                        # å¦‚æœå·²å­˜åœ¨è¯¥é¢‘é“ï¼Œä¿ç•™è´¨é‡æ›´å¥½çš„æº
                        if quality_score > stream_data[channel_name][1]:
                            stream_data[channel_name] = (line, quality_score)
                    else:
                        stream_data[channel_name] = (line, quality_score)
            else:
                # ä¿ç•™éURLè¡Œï¼ˆå¦‚åˆ†ç±»æ ‡é¢˜ï¼‰
                stream_data[line] = (line, float('inf'))  # ä½¿ç”¨æ— ç©·å¤§ç¡®ä¿æ ‡é¢˜è¡Œæ’åœ¨æœ€å‰é¢
    
    # æŒ‰è´¨é‡åˆ†æ•°æ’åºå¹¶å†™å…¥æ–‡ä»¶
    sorted_streams = sorted(stream_data.items(), key=lambda x: (-x[1][1], x[0]))  # æŒ‰åˆ†æ•°é™åºï¼Œé¢‘é“åå‡åº
    
    with open('live.txt', 'w', encoding='utf-8') as file:
        for _, (line, _) in sorted_streams:
            file.write(line + '\n')
    logging.info("Filtered and sorted content saved to live.txt")

if __name__ == "__main__":
    urls = [
        'https://raw.githubusercontent.com/kimwang1978/collect-tv-txt/main/merged_output.txt',
        'https://raw.githubusercontent.com/vbskycn/iptv/master/tv/iptv4.txt',
        'https://raw.githubusercontent.com/YueChan/Live/main/APTV.m3u',
        'http://ww.weidonglong.com/dsj.txt',
        'http://live.nctv.top/x.txt',
        'http://aktv.top/live.txt',
        'https://raw.githubusercontent.com/yuanzl77/IPTV/main/ç›´æ’­/å¤®è§†é¢‘é“.txt',
        'https://live.zhoujie218.top/tv/iptv4.txt',
        'https://raw.githubusercontent.com/Guovin/TV/gd/output/result.txt',
        'https://raw.githubusercontent.com/jiangnan1224/iptv_ipv4_live/refs/heads/main/live_ipv4.txt'
    ]
    fetch_and_filter(urls)