import requests
import logging
import subprocess
import re
from concurrent.futures import ThreadPoolExecutor
from collections import defaultdict
import time

# é…ç½®æ—¥å¿—è®°å½•
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def check_url_validity(url):
    try:
        response = requests.head(url, timeout=10)
        response.raise_for_status()
        return True
    except requests.exceptions.RequestException as e:
        logging.error(f"URL {url} is invalid or unreachable: {e}")
        return False

def fetch_content(url):
    try:
        logging.info(f"Fetching content from {url}")
        response = requests.get(url, timeout=10)
        response.raise_for_status()
        return response.content.decode('utf-8-sig')
    except requests.exceptions.RequestException as e:
        logging.error(f"Failed to fetch content from {url}: {e}")
        return None

def filter_content(content):
    if content is None:
        return []
    keywords = ["ãŠ™VIPæµ‹è¯•", "å…³æ³¨å…¬ä¼—å·", "å¤©å¾®ç§‘æŠ€", "è·å–æµ‹è¯•å¯†ç ", "æ›´æ–°æ—¶é—´", "â™¥èšç©ç›’å­", "ğŸŒ¹é˜²å¤±è”","ğŸ“¡  æ›´æ–°æ—¥æœŸ","ğŸ‘‰", 
                "ğŸ’“ä¸“äº«æºğŸ…°ï¸", "ğŸ’“ä¸“äº«æºğŸ…±ï¸", "å…³äºæœ¬æº", "æ¯æ—¥ä¸€é¦–", "MovieMusic", "AMCéŸ³ä¹", "å…¬å‘Š"]
    
    # é¦–å…ˆæŒ‰å…³é”®è¯è¿‡æ»¤
    filtered_lines = [line for line in content.splitlines() if not any(keyword in line for keyword in keywords)]
    
    # ç„¶åç§»é™¤æ¯è¡Œä¸­çš„emojiå’Œç‰¹æ®Šç¬¦å·
    emoji_pattern = re.compile("["
        u"\U0001F600-\U0001F64F"  # emoticons
        u"\U0001F300-\U0001F5FF"  # symbols & pictographs
        u"\U0001F680-\U0001F6FF"  # transport & map symbols
        u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
        u"\U00002702-\U000027B0"
        u"\U000024C2-\U0001F251"
        "]+", flags=re.UNICODE)
    
    cleaned_lines = []
    for line in filtered_lines:
        # ç§»é™¤emoji
        line = emoji_pattern.sub('', line)
        # å¦‚æœè¡Œä¸æ˜¯ç©ºçš„ï¼Œæ·»åŠ åˆ°ç»“æœä¸­
        if line.strip():
            cleaned_lines.append(line)
    
    return cleaned_lines

def measure_stream_speed(url):
    try:
        start_time = time.time()
        # ä½¿ç”¨ffmpegè·å–å‰3ç§’çš„æµæ•°æ®æ¥æµ‹è¯•é€Ÿåº¦
        command = ['ffmpeg', '-i', url, '-t', '3', '-f', 'null', '-']
        result = subprocess.run(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, timeout=20)
        end_time = time.time()
        
        if result.returncode == 0:
            # è¿”å›å“åº”æ—¶é—´ï¼ˆç§’ï¼‰
            return end_time - start_time
        return float('inf')  # å¦‚æœå¤±è´¥è¿”å›æ— ç©·å¤§
    except Exception:
        return float('inf')

def check_stream_validity(url):
    try:
        # ä½¿ç”¨ffmpegæ£€æŸ¥æµåª’ä½“æ˜¯å¦èƒ½æ­£å¸¸æ’­æ”¾
        command = ['ffmpeg', '-i', url, '-t', '10', '-f', 'null', '-']
        result = subprocess.run(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, timeout=20)
        if result.returncode == 0:
            return True
        else:
            logging.error(f"Stream {url} is not playable: {result.stderr.decode('utf-8')}")
            return False
    except subprocess.TimeoutExpired:
        logging.error(f"Stream {url} check timed out")
        return False
    except Exception as e:
        logging.error(f"Error checking stream {url}: {e}")
        return False

def extract_channel_name(line):
    # å°è¯•ä»è¡Œä¸­æå–é¢‘é“åç§°
    if ',' in line:  # m3uæ ¼å¼
        return line.split(',')[-1].strip()
    else:  # å…¶ä»–æ ¼å¼
        parts = line.split()
        if len(parts) > 1:
            # å‡è®¾URLåœ¨å‰ï¼Œåç§°åœ¨å
            return ' '.join(parts[1:]).strip()
    return None

def fetch_and_filter(urls):
    filtered_lines = []
    
    with ThreadPoolExecutor() as executor:
        # é¦–å…ˆæ£€æŸ¥URLçš„æœ‰æ•ˆæ€§
        valid_urls = [url for url in urls if check_url_validity(url)]
        # ç„¶åè·å–å†…å®¹
        results = list(executor.map(fetch_content, valid_urls))
    
    for content in results:
        filtered_lines.extend(filter_content(content))
    
    # æŒ‰é¢‘é“åç§°åˆ†ç»„å¹¶æµ‹è¯•é€Ÿåº¦
    channel_groups = defaultdict(list)
    valid_lines = []
    
    with ThreadPoolExecutor(max_workers=5) as executor:
        futures = []
        valid_streams = []
        
        # é¦–å…ˆæ£€æŸ¥æµçš„æœ‰æ•ˆæ€§
        for line in filtered_lines:
            if line.startswith('http'):
                url = line.split()[0]
                if check_stream_validity(url):
                    valid_streams.append(line)
            else:
                valid_streams.append(line)
        
        # å¯¹æœ‰æ•ˆçš„æµè¿›è¡Œé€Ÿåº¦æµ‹è¯•å’Œåˆ†ç»„
        for line in valid_streams:
            if line.startswith('http'):
                url = line.split()[0]
                channel_name = extract_channel_name(line)
                if channel_name:
                    speed = measure_stream_speed(url)
                    channel_groups[channel_name].append((speed, line))
            else:
                valid_lines.append(line)
    
    # å¯¹æ¯ä¸ªé¢‘é“ç»„å†…çš„æµæŒ‰é€Ÿåº¦æ’åº
    sorted_lines = []
    for channel_name, streams in channel_groups.items():
        # æŒ‰é€Ÿåº¦æ’åºï¼ˆå‡åºï¼‰
        sorted_streams = sorted(streams, key=lambda x: x[0])
        # åªæ·»åŠ æœ‰æ•ˆçš„æµï¼ˆé€Ÿåº¦ä¸æ˜¯æ— ç©·å¤§çš„ï¼‰
        sorted_lines.extend([stream[1] for stream in sorted_streams if stream[0] != float('inf')])
    
    # å°†éhttpè¡Œå’Œæ’åºåçš„æµåˆå¹¶
    final_lines = valid_lines + sorted_lines
    
    with open('live_ipv4.txt', 'w', encoding='utf-8') as file:
        file.write('\n'.join(final_lines))
    logging.info("Filtered and speed-sorted content saved to live_ipv4.txt")

if __name__ == "__main__":
    urls = [
        'https://raw.githubusercontent.com/kimwang1978/collect-tv-txt/main/merged_output.txt',
        'https://raw.githubusercontent.com/vbskycn/iptv/master/tv/iptv4.txt',
        'https://raw.githubusercontent.com/YueChan/Live/main/APTV.m3u',
        'http://ww.weidonglong.com/dsj.txt',
        'http://live.nctv.top/x.txt',
        'http://aktv.top/live.txt',
        'https://raw.githubusercontent.com/yuanzl77/IPTV/main/ç›´æ’­/å¤®è§†é¢‘é“.txt',
        'https://live.zhoujie218.top/tv/iptv4.txt',
        'https://raw.githubusercontent.com/Guovin/TV/gd/output/result.txt',
        'https://raw.githubusercontent.com/jiangnan1224/iptv_ipv4_live/refs/heads/main/live_ipv4.txt'
    ]
    fetch_and_filter(urls)